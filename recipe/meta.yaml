{% set version = "2.6.3" %}
{% set fused_dense_lib_name = "flash-attn-fused-dense-lib" %}
{% set layer_norm_name = "flash-attn-layer-norm" %}

package:
  name: flash-attn-split
  version: {{ version }}

source:
  - url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/flash_attn-{{ version }}.tar.gz
    sha256: 5bfae9500ad8e7d2937ebccb4906f3bc464d1bf66eedd0e4adabd520811c7b52
  # Overwrite with a simpler build script that doesn't try to revend pre-compiled binaries
  - path: pyproject.toml
  - path: setup.py

build:
  number: 2
  script_env:
    # Limit MAX_JOBS in order to prevent runners from crashing
    - MAX_JOBS=20
    - TORCH_CUDA_ARCH_LIST=8.0+PTX
  skip: true  # [cuda_compiler_version in (undefined, "None")]
  skip: true  # [not linux]
  skip: true  # [py==313]  # Skip until pytorch dependency on setuptools is fixed
  # debugging skips below
  # skip: true  # [py!=313]
  # skip: true  # [cuda_compiler_version != "12.0"]
  ignore_run_exports_from:
    - libcublas-dev    # [(cuda_compiler_version or "").startswith("12")]
    - libcusolver-dev  # [(cuda_compiler_version or "").startswith("12")]
    - libcusparse-dev  # [(cuda_compiler_version or "").startswith("12")]

requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}
    - {{ stdlib('c') }}
    - ninja
  host:
    - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build
    - cuda-cudart-dev  # [(cuda_compiler_version or "").startswith("12")]
    - libcublas-dev    # [(cuda_compiler_version or "").startswith("12")]
    - libcusolver-dev  # [(cuda_compiler_version or "").startswith("12")]
    - libcusparse-dev  # [(cuda_compiler_version or "").startswith("12")]
    - libtorch         # required until pytorch run_exports libtorch
    - pip
    - python
    - pytorch
    - pytorch =*=cuda*
    - setuptools
  run:
    - einops
    - python
    - pytorch =*=cuda*

outputs:
  - name: flash-attn
    build:
      script: python -m pip install . -vvv --no-deps --no-build-isolation

    test:
      imports:
        - flash_attn
      commands:
        - pip check
      requires:
        - pip

  - name: {{ fused_dense_lib_name|lower }}
    build:
      script: python -m pip install . -vvv --no-deps --no-build-isolation
      script_env:
        - BUILD_FLASH_ATTN_COMPONENT=fused_dense_lib

    test:
      imports:
        - fused_dense_lib
      commands:
        - pip check
      requires:
        - pip

  - name: {{ layer_norm_name|lower }}
    build:
      script: python -m pip install . -vvv --no-deps --no-build-isolation
      script_env:
        - BUILD_FLASH_ATTN_COMPONENT=layer_norm

    test:
      imports:
        - dropout_layer_norm
      commands:
        - pip check
      requires:
        - pip

about:
  home: https://github.com/Dao-AILab/flash-attention
  summary: 'Flash Attention: Fast and Memory-Efficient Exact Attention'
  license: BSD-3-Clause
  license_file:
    - LICENSE
    - LICENSE_CUTLASS.txt

extra:
  recipe-maintainers:
    - carterbox
    - weiji14
